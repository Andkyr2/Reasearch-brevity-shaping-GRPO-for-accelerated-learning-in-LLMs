{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aw2hlBMIHFWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e694658-3eb0-4aff-cc25-2fcfe42f7989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = '/content/drive/MyDrive/qwen_beta=0_001_E_5_epochs=3'\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G1MoqS5rHNrB"
      },
      "outputs": [],
      "source": [
        "\n",
        "#stopwatch placeholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rvq-mlRhHRMy"
      },
      "outputs": [],
      "source": [
        "LR = 3e-6\n",
        "CHECKPOINT = 'Qwen/Qwen2-0.5B-Instruct'\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtiW2mT3Hkn7",
        "outputId": "1cb381dc-de08-4f60-a2e7-1506e7d3056a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_da_WX8H-20",
        "outputId": "ddcfbee5-17a1-4c93-c15f-7a1a64a63a33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT,\n",
        "                                             dtype = torch.bfloat16).to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unOSNHTPIkyc",
        "outputId": "e6c7b38d-dc4e-4d98-a19e-b40bbfe1b309"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ref_model = AutoModelForCausalLM.from_pretrained(CHECKPOINT,\n",
        "                                             dtype = torch.bfloat16).to(device)\n",
        "ref_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHiXpFzIbeD",
        "outputId": "418564ae-79d3-4b9e-d0f6-0627fce5390a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "#freeze ref_model\n",
        "for param in ref_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(any(p.requires_grad for p in ref_model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1kaT_ZuzKJgk"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('gsm8k','main')['train']\n",
        "test_set = load_dataset('gsm8k','main')['test']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iwWFcmtDKhau"
      },
      "outputs": [],
      "source": [
        "def format_prompt(question):\n",
        "    system_message = 'Think step by step to answer the question given to you. After clearly stating your reasoning, state your final numerical answer after \"#### \". For example if the answer is 2 finish your answer with: #### 2. The final answer is always an integer with no spaces'\n",
        "    messages = [{'role':'system','content':system_message},\n",
        "                {'role':'user','content':question}]\n",
        "\n",
        "    formatted_message = tokenizer.apply_chat_template(messages,\n",
        "                                                     add_generation_prompt=True,\n",
        "                                                      tokenize = False)\n",
        "    return formatted_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bhBAFcT9LYNp"
      },
      "outputs": [],
      "source": [
        "def extract_answer(text):\n",
        "    pattern = r'#### (.*)'\n",
        "    matches = re.findall(pattern, text, re.DOTALL) #dotall is important\n",
        "    if matches:\n",
        "        try:\n",
        "            return int(matches[0])\n",
        "        except:\n",
        "            return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def map_func(example):\n",
        "    prompt = format_prompt(example['question'])\n",
        "    solution = example['answer']\n",
        "    answer = extract_answer(solution)\n",
        "\n",
        "    return {'prompt':prompt,'solution':solution,'answer':answer}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4yoRcslin3NJ"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(map_func)\n",
        "dataset = dataset.filter(lambda example: example['answer'] is not None)\n",
        "\n",
        "test_set = test_set.map(map_func)\n",
        "test_set = test_set.filter(lambda example: example['answer'] is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tMAotejhoDSC"
      },
      "outputs": [],
      "source": [
        "loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClymwEWKoGQf",
        "outputId": "fd912b4d-fb4a-4020-856b-d652c3eb8831"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(loader),len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_N-89m4Pok9X"
      },
      "outputs": [],
      "source": [
        "def generate_answers(prompts,num_return_sequences = 8,temp = 1,use_completion_dict = False):\n",
        "    '''\n",
        "    input:\n",
        "    prompts: List[str]\n",
        "    output:\n",
        "    completions: List[str] or Dict[str,List[str]]\n",
        "    '''\n",
        "\n",
        "\n",
        "    prompt_inputs = tokenizer(prompts,return_tensors='pt',padding_side='left',padding = True).to(device)\n",
        "    prompt_length = len(prompt_inputs['input_ids'][0])\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **prompt_inputs,\n",
        "        num_return_sequences = num_return_sequences,\n",
        "        max_new_tokens = 1024,\n",
        "        do_sample = True,\n",
        "        temperature = temp,\n",
        "        top_p = 0.95,\n",
        "    )\n",
        "    completions = tokenizer.batch_decode(outputs[:,prompt_length:],skip_special_tokens=True)\n",
        "    if use_completion_dict:\n",
        "        completion_dict = {}\n",
        "        for prompt_num,i in enumerate(range(0,len(completions),num_return_sequences)):\n",
        "            completion_dict[prompts[prompt_num]] = completions[i:i+num_return_sequences]\n",
        "        return completion_dict\n",
        "\n",
        "    return completions\n",
        "\n",
        "def extract_reasoning(text):\n",
        "    splitted = text.split('####')\n",
        "    return splitted[0]\n",
        "\n",
        "def calculate_rewards(rollouts,real_answer,R = None):\n",
        "    rewards = []\n",
        "    if R == 'penalty':\n",
        "        rollouts.sort(key=lambda x: len(tokenizer.encode(extract_reasoning(x))),reverse=True)\n",
        "    elif R == 'reward':\n",
        "        rollouts.sort(key=lambda x: len(tokenizer.encode(extract_reasoning(x))),reverse=False)\n",
        "\n",
        "    for i,rollout in enumerate(rollouts):\n",
        "        try:\n",
        "            ans = extract_answer(rollout)\n",
        "        except:\n",
        "            ans = None\n",
        "\n",
        "\n",
        "        if ans == None:\n",
        "            rewards.append(-0.1)\n",
        "\n",
        "        elif ans == real_answer:\n",
        "            if R == None:\n",
        "                rewards.append(1)\n",
        "            else:\n",
        "                rewards.append((i+1)/len(rollouts))\n",
        "        else:\n",
        "            rewards.append(0)\n",
        "    return rewards\n",
        "\n",
        "def calculate_advantages(rewards):\n",
        "    advantages = []\n",
        "    mean = sum(rewards)/len(rewards)\n",
        "    # std = (sum([(reward-mean)**2 for reward in rewards])/len(rewards))**0.5\n",
        "    # if std == 0:\n",
        "    #     std = 1\n",
        "    std = 1\n",
        "    for reward in rewards:\n",
        "        advantage = (reward-mean)/std\n",
        "        advantages.append(advantage)\n",
        "    return advantages\n",
        "\n",
        "def is_correct(ans,real_ans):\n",
        "    if ans == real_ans:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def calculate_accuracy(rollouts,real_answer):\n",
        "    correct = 0\n",
        "    for rollout in rollouts:\n",
        "        if extract_answer(rollout) == real_answer:\n",
        "            correct += 1\n",
        "    return correct/len(rollouts)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hXrlZ8jE7ARV"
      },
      "outputs": [],
      "source": [
        "def get_log_probs(model,prompt,completions,strategy = 'sum'):\n",
        "    '''\n",
        "    input:\n",
        "    prompt: str\n",
        "    completions: List[str]\n",
        "    output:\n",
        "    log_probs: torch.tensor[float]\n",
        "    '''\n",
        "    prompt_length = len(tokenizer.encode(prompt))\n",
        "    prompt_completions = [prompt+completion for completion in completions]\n",
        "\n",
        "    prompt_inputs = tokenizer(prompt_completions,return_tensors='pt',padding_side='right',padding = True).to(device)\n",
        "    completion_ids = prompt_inputs['input_ids'][:,prompt_length:]\n",
        "\n",
        "    logits = model(**prompt_inputs).logits\n",
        "    completion_logits = logits[:,prompt_length-1:-1,:]\n",
        "    log_probs = torch.nn.functional.log_softmax(completion_logits,dim=-1)\n",
        "    log_probs = torch.gather(log_probs,-1,completion_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    mask = prompt_inputs['attention_mask'][:,prompt_length:]\n",
        "    log_probs = log_probs*mask\n",
        "\n",
        "    if strategy == 'sum':\n",
        "        log_probs = log_probs.sum(dim=-1)\n",
        "\n",
        "    elif strategy == 'mean':\n",
        "        summed_log_probs = log_probs.sum(dim=-1)\n",
        "        num_tokens = mask.sum(dim=-1)\n",
        "        log_probs = summed_log_probs/num_tokens\n",
        "\n",
        "    elif strategy == 'list':\n",
        "        N = log_probs.shape[0]\n",
        "        list_of_tensors = []\n",
        "        non_masked_token_number = mask.sum(dim = -1)\n",
        "        for i in range(N):\n",
        "            list_of_tensors.append(log_probs[i,:non_masked_token_number[i]])\n",
        "\n",
        "        log_probs = list_of_tensors\n",
        "\n",
        "\n",
        "    return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2dC0lxqD7AId"
      },
      "outputs": [],
      "source": [
        "#deprecated\n",
        "\n",
        "# def calculate_loss(prompt,completions,real_answer,R = None,beta = 0):\n",
        "#     rewards = calculate_rewards(completions,real_answer,R = R)\n",
        "#     advantages = torch.tensor(calculate_advantages(rewards)).to(device)\n",
        "#     total_objective = 0\n",
        "\n",
        "#     log_probs_summed = get_log_probs(model,prompt,completions,strategy = 'sum')\n",
        "#     total_objective += (log_probs_summed*advantages).mean()\n",
        "\n",
        "#     log_probs_ref_meanned = get_log_probs(ref_model,prompt,completions,strategy = 'mean')\n",
        "#     log_probs_meanned = get_log_probs(model,prompt,completions,strategy = 'mean')\n",
        "\n",
        "#     kl_div = (log_probs_meanned-log_probs_ref_meanned).mean()\n",
        "\n",
        "#     return -(total_objective/1024.0 - beta*kl_div),kl_div.item()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AuFWXkQcRaqP"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(prompt,completions,real_answer,R = None,beta = 0):\n",
        "    rewards = calculate_rewards(completions,real_answer,R = R)\n",
        "    advantages = torch.tensor(calculate_advantages(rewards)).to(device)\n",
        "    total_objective = 0\n",
        "\n",
        "    list_of_log_prob_tensors = get_log_probs(model,prompt,completions,strategy = 'list')\n",
        "    list_of_log_prob_sums = [t.sum() for t in list_of_log_prob_tensors]\n",
        "    log_probs_summed = torch.stack(list_of_log_prob_sums).to(device)\n",
        "    total_objective += (log_probs_summed*advantages).mean()\n",
        "\n",
        "    ref_list_of_log_prob_tensors = get_log_probs(ref_model,prompt,completions,strategy='list')\n",
        "\n",
        "    ref_big_tensor = torch.concat(ref_list_of_log_prob_tensors)\n",
        "    big_tensor = torch.concat(list_of_log_prob_tensors)\n",
        "    def kl_estimator(log_pi_theta,log_pi_ref):\n",
        "        r = log_pi_ref - log_pi_theta\n",
        "        estimator = torch.exp(r) - r - 1\n",
        "        return estimator\n",
        "    kl_div = kl_estimator(big_tensor,ref_big_tensor).mean()\n",
        "\n",
        "    N = len(big_tensor)\n",
        "\n",
        "    return -(total_objective/N - beta*kl_div),kl_div.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2DuTMWZgwv",
        "outputId": "c2e80437-1bba-4fc5-8a11-b51245e1a8dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1763050466.4828677"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-fgFVzsmFeSJ"
      },
      "outputs": [],
      "source": [
        "def pass_at_N(completions,answer):\n",
        "    for completion in completions:\n",
        "        if extract_answer(completion) == answer:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9LBuWvqPR9u",
        "outputId": "42603412-ae15-4f48-8a4b-cc3b5d99ef91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experiment 0\n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:0\n",
            "                Accuracy:0.04296875\n",
            "                Moving Average Accuracy:0.04296875\n",
            "                Average Length:427.234375\n",
            "                Average Length With Prompt:555.875\n",
            "                Time Taken:151.08661723136902\n",
            "                Time Taken per average_word: = 0.35363871933612323\n",
            "                Time Taken per average_word_with_prompt: = 0.2717996262313812\n",
            "                Moving Average Length:427.234375\n",
            "                Average KL Divergence:0.0\n",
            "                Moving Average KL Divergence:0.0\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:1\n",
            "                Accuracy:0.060546875\n",
            "                Moving Average Accuracy:0.0517578125\n",
            "                Average Length:391.1875\n",
            "                Average Length With Prompt:515.40625\n",
            "                Time Taken:147.4160213470459\n",
            "                Time Taken per average_word: = 0.3768423616476649\n",
            "                Time Taken per average_word_with_prompt: = 0.28601907979782143\n",
            "                Moving Average Length:409.2109375\n",
            "                Average KL Divergence:0.001137852668762207\n",
            "                Moving Average KL Divergence:0.0005689263343811035\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:2\n",
            "                Accuracy:0.052734375\n",
            "                Moving Average Accuracy:0.052083333333333336\n",
            "                Average Length:383.484375\n",
            "                Average Length With Prompt:509.875\n",
            "                Time Taken:97.59913778305054\n",
            "                Time Taken per average_word: = 0.254506165428645\n",
            "                Time Taken per average_word_with_prompt: = 0.19141777451934402\n",
            "                Moving Average Length:400.6354166666667\n",
            "                Average KL Divergence:0.0020723938941955566\n",
            "                Moving Average KL Divergence:0.0010700821876525879\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:3\n",
            "                Accuracy:0.0703125\n",
            "                Moving Average Accuracy:0.056640625\n",
            "                Average Length:383.0625\n",
            "                Average Length With Prompt:507.9375\n",
            "                Time Taken:143.45481038093567\n",
            "                Time Taken per average_word: = 0.37449452864985655\n",
            "                Time Taken per average_word_with_prompt: = 0.2824261063239782\n",
            "                Moving Average Length:396.2421875\n",
            "                Average KL Divergence:0.002942323684692383\n",
            "                Moving Average KL Divergence:0.0015381425619125366\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:4\n",
            "                Accuracy:0.107421875\n",
            "                Moving Average Accuracy:0.066796875\n",
            "                Average Length:340.296875\n",
            "                Average Length With Prompt:469.84375\n",
            "                Time Taken:147.0880606174469\n",
            "                Time Taken per average_word: = 0.43223453232547876\n",
            "                Time Taken per average_word_with_prompt: = 0.31305739539463256\n",
            "                Moving Average Length:385.053125\n",
            "                Average KL Divergence:0.004000425338745117\n",
            "                Moving Average KL Divergence:0.002030599117279053\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:5\n",
            "                Accuracy:0.107421875\n",
            "                Moving Average Accuracy:0.07356770833333333\n",
            "                Average Length:415.484375\n",
            "                Average Length With Prompt:546.40625\n",
            "                Time Taken:152.95040774345398\n",
            "                Time Taken per average_word: = 0.3681255347892541\n",
            "                Time Taken per average_word_with_prompt: = 0.27992067759739936\n",
            "                Moving Average Length:390.125\n",
            "                Average KL Divergence:0.006258964538574219\n",
            "                Moving Average KL Divergence:0.0027353266874949136\n",
            "                \n",
            "\n",
            "\n",
            "                Epoch:0\n",
            "                Batch Num:6\n",
            "                Accuracy:0.11328125\n",
            "                Moving Average Accuracy:0.07924107142857142\n",
            "                Average Length:312.953125\n",
            "                Average Length With Prompt:438.984375\n",
            "                Time Taken:94.07255244255066\n",
            "                Time Taken per average_word: = 0.3005963031765561\n",
            "                Time Taken per average_word_with_prompt: = 0.2142959016310106\n",
            "                Moving Average Length:379.10044642857144\n",
            "                Average KL Divergence:0.010170698165893555\n",
            "                Moving Average KL Divergence:0.003797522612980434\n",
            "                \n"
          ]
        }
      ],
      "source": [
        "for experiment in range(3):\n",
        "    print(f'experiment {experiment}')\n",
        "    #seed\n",
        "    torch.manual_seed(42+experiment)\n",
        "\n",
        "    #models\n",
        "    model = AutoModelForCausalLM.from_pretrained(CHECKPOINT,\n",
        "                                                 dtype = torch.bfloat16).to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "    test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(),lr=LR)\n",
        "\n",
        "    EPOCHS = 3\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        batch_kl_divs = []\n",
        "        batch_accuracies = []\n",
        "        batch_average_lengths = []\n",
        "        batch_average_lengths_with_prompts = []\n",
        "        batch_time_taken = []\n",
        "\n",
        "\n",
        "\n",
        "        for batch_num,batch in enumerate(loader):\n",
        "\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                if batch_num <len(loader)*0.7 or epoch <= 1:\n",
        "                    R = 'penalty'\n",
        "                elif batch_num < len(loader)*0.7 + 5:\n",
        "                    R = 'reward'\n",
        "                else:\n",
        "                    R = None\n",
        "\n",
        "                prompts = batch['prompt']\n",
        "                answers = batch['answer']\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    completion_dict = generate_answers(prompts,use_completion_dict = True)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                max_lengths = []\n",
        "                max_lengths_with_promps = []\n",
        "                accuracies = []\n",
        "                kl_divs = []\n",
        "\n",
        "                for prompt,real_answer in zip(prompts,answers):\n",
        "\n",
        "                    completions = completion_dict[prompt]\n",
        "                    loss,kl_div = calculate_loss(prompt,completions,real_answer,R = R,beta = 0.001)\n",
        "\n",
        "                    loss /= len(completions)\n",
        "                    loss.backward()\n",
        "\n",
        "                    kl_divs.append(kl_div)\n",
        "\n",
        "                    lengths = [len(tokenizer.encode(completion)) for completion in completions]\n",
        "                    lengths_with_prompts = [len(tokenizer.encode(prompt+completion)) for completion in completions]\n",
        "                    max_lengths.append(max(lengths))\n",
        "                    max_lengths_with_promps.append(max(lengths_with_prompts))\n",
        "\n",
        "\n",
        "                    are_correct = [is_correct(extract_answer(completion),real_answer) for completion in completions]\n",
        "                    accuracies.append(sum(are_correct)/len(are_correct))\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                end_time = time.time()\n",
        "                batch_time_taken.append(end_time-start_time)\n",
        "                batch_accuracies.append(sum(accuracies)/len(accuracies))\n",
        "                batch_average_lengths.append(sum(max_lengths)/len(max_lengths))\n",
        "                batch_average_lengths_with_prompts.append(sum(max_lengths_with_promps)/len(max_lengths_with_promps))\n",
        "                batch_kl_divs.append(sum(kl_divs)/len(kl_divs))\n",
        "                status_for_print = f'''\n",
        "\n",
        "                Epoch:{epoch}\n",
        "                Batch Num:{batch_num}\n",
        "                Accuracy:{sum(accuracies)/len(accuracies)}\n",
        "                Moving Average Accuracy:{sum(batch_accuracies[-10:])/len(batch_accuracies[-10:])}\n",
        "                Average Length:{batch_average_lengths[-1]}\n",
        "                Average Length With Prompt:{batch_average_lengths_with_prompts[-1]}\n",
        "                Time Taken:{batch_time_taken[-1]}\n",
        "                Time Taken per average_word: = {batch_time_taken[-1]/batch_average_lengths[-1]}\n",
        "                Time Taken per average_word_with_prompt: = {batch_time_taken[-1]/batch_average_lengths_with_prompts[-1]}\n",
        "                Moving Average Length:{sum(batch_average_lengths[-10:])/len(batch_average_lengths[-10:])}\n",
        "                Average KL Divergence:{sum(kl_divs)/len(kl_divs)}\n",
        "                Moving Average KL Divergence:{sum(batch_kl_divs[-10:])/len(batch_kl_divs[-10:])}\n",
        "                '''\n",
        "                print(status_for_print)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(f'Batch Num:{batch_num}')\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Sample Completions for experiment {experiment} are:')\n",
        "    for i,completion in enumerate(completions):\n",
        "        print(f'{i+1}. {completion}')\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.subplots(3,1,figsize=(15,20))\n",
        "    plt.subplot(3,1,1)\n",
        "    plt.plot(batch_accuracies)\n",
        "    plt.title('Accuracy')\n",
        "    plt.subplot(3,1,2)\n",
        "    plt.plot(batch_average_lengths)\n",
        "    plt.title('Average Length')\n",
        "    plt.subplot(3,1,3)\n",
        "    plt.plot(batch_kl_divs)\n",
        "    plt.title('KL Divergence')\n",
        "    plt.show()\n",
        "\n",
        "    df_for_save = pd.DataFrame({'batch_accuracies':batch_accuracies,'batch_average_lengths':batch_average_lengths,'batch_kl_divs':batch_kl_divs,'batch_time_taken':batch_time_taken,'batch_average_lengths_with_prompts':batch_average_lengths_with_prompts})\n",
        "    df_for_save.to_csv(f'{save_path}/experiment_{experiment}.csv')\n",
        "\n",
        "    #eval\n",
        "    num_correct_answers = 0\n",
        "    total_answers = 0\n",
        "    for batch_num,batch in enumerate(test_loader):\n",
        "\n",
        "        try:\n",
        "            prompts = batch['prompt']\n",
        "            real_answers = batch['answer']\n",
        "            solutions = batch['solution']\n",
        "\n",
        "            with torch.no_grad():\n",
        "                completion_dict = generate_answers(prompts,use_completion_dict=True)\n",
        "\n",
        "                for prompt,real_answer in zip(prompts,real_answers):\n",
        "                    completions = completion_dict[prompt]\n",
        "                    correctness = pass_at_N(completions,real_answer)\n",
        "                    total_answers += 1\n",
        "                    if correctness:\n",
        "                        num_correct_answers += 1\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(f'Batch Num:{batch_num}')\n",
        "    print(f'Test Accuracy for experiment {experiment} is {num_correct_answers/total_answers}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKx3p7_TZ48G"
      },
      "outputs": [],
      "source": [
        "completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qYV6NL5znGy"
      },
      "outputs": [],
      "source": [
        "A = torch.tensor([1,2,3],dtype=torch.float32)\n",
        "B = torch.tensor([4,5,6],dtype=torch.float32)\n",
        "A.requires_grad = True\n",
        "B.requires_grad = True\n",
        "A\n",
        "l = [A,B]\n",
        "l_summed = [t.sum() for t in l]\n",
        "l_summed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuinF7U7fApp"
      },
      "outputs": [],
      "source": [
        "_"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}